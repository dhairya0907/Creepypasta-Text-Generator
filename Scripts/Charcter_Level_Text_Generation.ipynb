{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Charcter_Level_Text_Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dhairya0907/Creepypasta-Text-Generator/blob/main/Scripts/Charcter_Level_Text_Generation.ipynb)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%capture\n",
        "!pip install wandb -qqq"
      ],
      "outputs": [],
      "metadata": {
        "id": "dwqUXwZY3Jrt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%load_ext tensorboard"
      ],
      "outputs": [],
      "metadata": {
        "id": "zF5Ny2EC3M2a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import tensorflow.compat.v2 as tf\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "\n",
        "tf.enable_v2_behavior()\n",
        "disable_eager_execution()"
      ],
      "outputs": [],
      "metadata": {
        "id": "MaVBXosm3Oaf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\n",
        "import io\n",
        "import time\n",
        "import wandb\n",
        "import random\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from wandb import AlertLevel\n",
        "from keras.layers import RNN\n",
        "from tensorflow import keras\n",
        "from keras.layers import LSTM\n",
        "from google.colab import drive\n",
        "from keras.layers import Dense\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import Dropout\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Sequential\n",
        "from wandb.keras import WandbCallback\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "outputs": [],
      "metadata": {
        "id": "tZRSiDYNQLSr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "drive.mount('/content/drive')"
      ],
      "outputs": [],
      "metadata": {
        "id": "oOlbCvar4Dy_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "wandb.login()"
      ],
      "outputs": [],
      "metadata": {
        "id": "M5Pud3Hi4HBb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "Epochs = 1\n",
        "Batch = 128\n",
        "time_folder = \"First\"\n",
        "story_length = 100\n",
        "vocab_Max = 34\n",
        "vocab_Orignal = 0\n",
        "main_folder_name = \"\" # Folder name which have all the files.\n",
        "file_name  = \"\" # File name on which you want to train model."
      ],
      "outputs": [],
      "metadata": {
        "id": "cs27nSly4OrF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "folder_file = file_name.split(\"/\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "1IiOg4JB4RPd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "text = (open(\"/content/drive/My Drive/path_to_file\").read())\n",
        "text = text.lower()"
      ],
      "outputs": [],
      "metadata": {
        "id": "87T5BetZQYQv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import nltk\n",
        "nltk.download('words')\n",
        "words = set(nltk.corpus.words.words())\n",
        "\n",
        "\n",
        "text = \" \".join(w for w in nltk.wordpunct_tokenize(text) \\\n",
        "         if w.lower() in words or not w.isalpha())"
      ],
      "outputs": [],
      "metadata": {
        "id": "H_R_HG6DWvHQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "text = ''.join([i for i in text if not i.isdigit()])"
      ],
      "outputs": [],
      "metadata": {
        "id": "LJxXl0EhWzM_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "encoded_string = text.encode(\"ascii\", \"ignore\")\n",
        "text = encoded_string.decode()"
      ],
      "outputs": [],
      "metadata": {
        "id": "E_rXTu9hW2Bp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "bad_chars = ['~', '|', '\\x11', '#', '$', '%','(', ')', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>','@', '\\\\', '^', '_', '`',]\n",
        " \n",
        "# using replace() to\n",
        "# remove bad_chars\n",
        "for i in bad_chars :\n",
        "    text = text.replace(i, '')"
      ],
      "outputs": [],
      "metadata": {
        "id": "bOzCldxhW4WP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "characters = sorted(list(set(text)))\n",
        "\n",
        "n_to_char = {n:char for n, char in enumerate(characters)}\n",
        "char_to_n = {char:n for n, char in enumerate(characters)}\n",
        "\n",
        "vocab_size = len(characters)\n",
        "print('Number of unique characters: ', vocab_size)\n",
        "print(characters)"
      ],
      "outputs": [],
      "metadata": {
        "id": "YOckP_sYQcda"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "vocab_Orignal = vocab_size"
      ],
      "outputs": [],
      "metadata": {
        "id": "V3vuymxt9Ek4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Optional\n",
        "\n",
        "experiment_name = \"\"\n",
        "\n",
        "wandb.init(project=\"\", \n",
        "           name=folder_file[2],\n",
        "           sync_tensorboard=True,\n",
        "           group=experiment_name,\n",
        "           notes=\"\",\n",
        "           tags=[\"\", \"\", \"\",\"\",\"\"])\n",
        "\n",
        "\n",
        "artifact = wandb.Artifact('', type='dataset')\n",
        "artifact.add_file(\"/content/drive/My Drive/path_to_file\")\n",
        "wandb.log_artifact(artifact)\n",
        "\n",
        "wandb.config.update({\"epochs\": Epochs, \n",
        "                     \"batch_size\": Batch,\n",
        "                     \"architecture\": \"RNN\",\n",
        "                     \"dataset\": file_name,\n",
        "                     \"activation\": \"softmax\",\n",
        "                     \"optimizer\": \"Adam\",\n",
        "                     \"loss\": \"categorical_crossentropy\",\n",
        "                     \"metric\": \"accuracy\",\n",
        "                     \"vocab_Max\": vocab_Max,\n",
        "                     \"vocab_Orignal\" : vocab_Orignal,\n",
        "                     })\n",
        "\n",
        "\n",
        "WANDB_DISABLE_CODE= \"true\""
      ],
      "outputs": [],
      "metadata": {
        "id": "4lVluPFB4TzP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "last = [' ', '!', '\"', '&', \"'\", '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}'] # Vocab in last file\n",
        "current =[' ', '!', '\"', '&', \"'\", '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}'] # Vocab in current file\n",
        "\n",
        "# Both should be same"
      ],
      "outputs": [],
      "metadata": {
        "id": "PwzUkKKw3uzS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(len(last))\n",
        "print(len(current))"
      ],
      "outputs": [],
      "metadata": {
        "id": "1LaQjWk84Ttk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "list(set(current) - set(last))"
      ],
      "outputs": [],
      "metadata": {
        "id": "OXJzsuIv9t9f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "bad_chars = \"\"\n",
        "# using replace() to\n",
        "# remove bad_chars\n",
        "for i in bad_chars :\n",
        "    text = text.replace(i, '')"
      ],
      "outputs": [],
      "metadata": {
        "id": "1njc-krLib8E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "list(set(last) - set(current))"
      ],
      "outputs": [],
      "metadata": {
        "id": "ltShb3i44Npp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "text = text + '}' + '{'"
      ],
      "outputs": [],
      "metadata": {
        "id": "mU8vVwrHAVO-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "characters = sorted(list(set(text)))\n",
        "\n",
        "n_to_char = {n:char for n, char in enumerate(characters)}\n",
        "char_to_n = {char:n for n, char in enumerate(characters)}\n",
        "\n",
        "vocab_size = len(characters)\n",
        "print('Number of unique characters: ', vocab_size)\n",
        "print(characters)"
      ],
      "outputs": [],
      "metadata": {
        "id": "5JFrSxvp37Sp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import re\n",
        "res = re.sub(' +', ' ', text)"
      ],
      "outputs": [],
      "metadata": {
        "id": "QT48SADLjlnV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "characters = sorted(list(set(text)))\n",
        "\n",
        "n_to_char = {n:char for n, char in enumerate(characters)}\n",
        "char_to_n = {char:n for n, char in enumerate(characters)}\n",
        "\n",
        "vocab_size = len(characters)\n",
        "print('Number of unique characters: ', vocab_size)\n",
        "print(characters)"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZfHt5jMIju1e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X = []   # extracted sequences\n",
        "Y = []   # the target: follow up character for each sequence in X\n",
        "length = len(text)\n",
        "seq_length = 100\n",
        "\n",
        "for i in range(0, length - seq_length, 1):\n",
        "    sequence = text[i:i + seq_length]\n",
        "    label = text[i + seq_length]\n",
        "    X.append([char_to_n[char] for char in sequence])\n",
        "    Y.append(char_to_n[label])\n",
        "    \n",
        "print('Number of extracted sequences:', len(X))"
      ],
      "outputs": [],
      "metadata": {
        "id": "_F9FfyFSQl_5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_modified = np.reshape(X, (len(X), seq_length, 1))\n",
        "X_modified = X_modified / float(len(characters))\n",
        "Y_modified = np_utils.to_categorical(Y)"
      ],
      "outputs": [],
      "metadata": {
        "id": "xalsFDNyQp66"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_modified.shape, Y_modified.shape"
      ],
      "outputs": [],
      "metadata": {
        "id": "TpOsbrmXAQD5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(700, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(700, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(700))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(Y_modified.shape[1], activation='softmax'))"
      ],
      "outputs": [],
      "metadata": {
        "id": "XryjXIYhQ_gM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model.summary()"
      ],
      "outputs": [],
      "metadata": {
        "id": "haSXZSjG98mj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "filename = \"/content/drive/My Drive/path_to_last_best_model\"\n",
        "model.load_weights(filename) # After first run, for incremental training\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "\n",
        "filepath = \"/content/drive/My Drive/Machine Learning Research/path_to_current_model/\"+time_folder+\"/\"+os.path.splitext(folder_file[2])[0]+\"-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "callbacks_list = [checkpoint, WandbCallback(), tensorboard_callback]\n",
        "\n",
        "#model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
      ],
      "outputs": [],
      "metadata": {
        "id": "beJ_3jq4JkDD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Optional\n",
        "\n",
        "wandb.config.update({\"layers\" : len(model.layers),})"
      ],
      "outputs": [],
      "metadata": {
        "id": "ojA77PFs96Aq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "start_time = time.time()\n",
        "\n",
        "model.fit(X_modified, Y_modified, epochs=Epochs, batch_size=Batch, callbacks = callbacks_list)\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "outputs": [],
      "metadata": {
        "id": "kbg5pOhxOKbY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "time_in_min = (time.time() - start_time)/60\n",
        "print(time_in_min)"
      ],
      "outputs": [],
      "metadata": {
        "id": "gUrbdr4651qW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "time_in_min = round(time_in_min, 2)\n",
        "print(time_in_min)"
      ],
      "outputs": [],
      "metadata": {
        "id": "9CcpAIRi545Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "time_in_hr = round(time_in_min, 2)/60\n",
        "print(time_in_hr)"
      ],
      "outputs": [],
      "metadata": {
        "id": "HkjjBnK456h2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "time_in_hr = round(time_in_hr, 2)\n",
        "print(time_in_hr)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Z-Q-iUNG58FR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Optional\n",
        "\n",
        "wandb.config.update({\"fitting_time\" : str(time_in_hr)+\"h\"})"
      ],
      "outputs": [],
      "metadata": {
        "id": "fupXiRCy5-Vn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#!kill 3639\n",
        "%tensorboard --logdir logs/fit"
      ],
      "outputs": [],
      "metadata": {
        "id": "PWtcIvk76AIY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# load weights form best model just tranined\n",
        "\n",
        "filename = \"\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
      ],
      "outputs": [],
      "metadata": {
        "id": "c4aFgiSz6D10"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model.save(\"/content/drive/My Drive/path_to_final_model_for_this_run\"+str(time_folder)+\"/\"+str(os.path.splitext(folder_file[2])[0])+\"_final_model.hdf5\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "1kxhCW0g6JVE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model.save(\"/content/drive/My Drive/path_to_final_model_for_this_run_for_algorithmia\"+str(time_folder)+\"/\"+str(os.path.splitext(folder_file[2])[0])+\"_algorithmia_model.h5\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "L2YGt9P56LHC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "start = np.random.randint(0, len(X)-1) # or generate random start\n",
        "\n",
        "string_mapped = list(X[start])\n",
        "\n",
        "full_string = [n_to_char[value] for value in string_mapped]\n",
        "\n",
        "\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join(full_string), \"\\\"\")\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "H_tBESKCrBM2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "filename = \"/content/drive/My Drive/path_to_store_genrated_story\"\n",
        "if not os.path.exists(os.path.dirname(filename)):\n",
        "  try:\n",
        "    os.makedirs(os.path.dirname(filename))\n",
        "  except OSError as exc:\n",
        "    if exc.errno != errno.EEXIST:\n",
        "      raise\n",
        "\n",
        "f = open(filename, \"a\")\n",
        "f.write(str(filename)+\" : \")\n",
        "f.write(\"\\n\")\n",
        "f.write(\"\\n\")\n",
        "f.write(str(\"First\")+\" : \")\n",
        "f.write(\"\\n\")\n",
        "f.write(\"\\n\")\n",
        "f.write(\"Number of words : \\n\" + str(story_length))\n",
        "f.write(\"\\n\")\n",
        "f.write(\"\\n\")\n",
        "s = \"\\\"\", ''.join(full_string), \"\\\"\"\n",
        "f.write(\"Starting String : \\n \"+str(s))\n",
        "f.write(\"\\n\")\n",
        "f.write(\"\\n\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "ptsTn7mfinrw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def sample(preds, temperature=0.5):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "outputs": [],
      "metadata": {
        "id": "_hS47qp6P2F3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "# generating characters\n",
        "for i in range(story_length):\n",
        "\n",
        "  x = np.reshape(string_mapped, (1, len(string_mapped), 1))\n",
        "  x = (x / float(len(characters))) + (np.random.rand(1, len(string_mapped), 1) * 0.01)\n",
        "  prediction = model.predict(x, verbose=0)\n",
        "  pred_index = sample(np.ndarray.flatten(prediction))\n",
        "  seq = [n_to_char[value] for value in string_mapped]\n",
        "  full_string.append(n_to_char[pred_index])\n",
        "  string_mapped.append(pred_index)  # add the predicted character to the end\n",
        "  string_mapped = string_mapped[1:len(string_mapped)] # shift the string one character forward by removing pos. 0"
      ],
      "outputs": [],
      "metadata": {
        "id": "vx6jgsrQBRSp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "txt=\"\"\n",
        "for char in full_string:\n",
        "    txt = txt+char"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZqJQiYthXPbB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "f.write(\"Story : \\n\")\n",
        "f.write(txt)\n",
        "f.write(\"\\n\")\n",
        "f.write(\"\\n\")\n",
        "f.write(\"\\n\")\n",
        "f.write(\"\\n\")\n",
        "f.close()"
      ],
      "outputs": [],
      "metadata": {
        "id": "bWaufIzO68sC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(start)\n",
        "print(txt)"
      ],
      "outputs": [],
      "metadata": {
        "id": "EM-I1QV4XRAS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Optional\n",
        "\n",
        "trained_model_artifact = wandb.Artifact(\n",
        "            str(os.path.splitext(\"path_to_model\", type=\"model\",description=\"\",)))\n",
        "\n",
        "\n",
        "trained_model_artifact.add_file(\"/content/drive/My Drive/path_to_algorithmia_model\"+str(os.path.splitext(folder_file[2])[0])+\"_algorithmia_model.h5\")\n",
        "wandb.log_artifact(trained_model_artifact)"
      ],
      "outputs": [],
      "metadata": {
        "id": "MqN49ZSG6cRq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Optional\n",
        "\n",
        "wandb.alert(\n",
        "      title='Run  '+main_folder_name+'  Finished',\n",
        "      text=f'The Run of file {file_name} is completed in {(time.time() - start_time)} seconds.',\n",
        "      level=AlertLevel.INFO,\n",
        "      wait_duration= datetime.timedelta(minutes=0)\n",
        "    )"
      ],
      "outputs": [],
      "metadata": {
        "id": "pPRePikS6f8c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Optional\n",
        "\n",
        "wandb.finish()"
      ],
      "outputs": [],
      "metadata": {
        "id": "un6iyzQ66hdL"
      }
    }
  ]
}